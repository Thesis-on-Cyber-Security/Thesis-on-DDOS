{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<center><h2>\n",
    "<a href=\"https://masumbhai.me\">Kindly visit my portfolio to see more of my works</a>\n",
    "</h2></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color: #0FFF50\">\n",
    "anaconda login <br/>\n",
    "conda install -c conda-forge numpy <br/>\n",
    "pip install pandas <br/>\n",
    "pip install seaborn <br/>\n",
    "conda install -c conda-forge matplotlib <br/>\n",
    "conda install -c conda-forge scikit-learn <br/>\n",
    "conda install -c conda-forge dask <br/>\n",
    "conda install -c fastai fastcore <br/>\n",
    "conda install jupyter notebook <br/>\n",
    "pip install jupyter <br/>\n",
    "pip install tqdm <br/>\n",
    "conda install cudatoolkit <br/>\n",
    "pip install --default-timeout=100 tensorflow==2.3.4 <br/>\n",
    "pip install --default-timeout=100 tensorflow-gpu==2.3.4 <br/>\n",
    "pip install numba <br/>\n",
    "</code>\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dask import dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h4 style=\"background:yellow;color:black\">\n",
    "Setting up paths to csv files / datasets\n",
    "</center></h4>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CSV-01-12\n",
    "path_DdoS_DNS = \"D:\\\\thesis_dataset\\\\01-12\\\\DrDoS_DNS.csv\"\n",
    "path_DdoS_MSSQL = \"D:\\\\thesis_dataset\\\\01-12\\\\DrDoS_MSSQL.csv\"\n",
    "path_DdoS_LDAP = \"D:\\\\thesis_dataset\\\\01-12\\\\DrDoS_LDAP.csv\"\n",
    "path_DdoS_NTP = \"D:\\\\thesis_dataset\\\\01-12\\\\DrDoS_NTP.csv\"\n",
    "path_DdoS_NetBIOS = \"D:\\\\thesis_dataset\\\\01-12\\\\DrDoS_NetBIOS.csv\"\n",
    "path_DdoS_SNMP = \"D:\\\\thesis_dataset\\\\01-12\\\\DrDoS_SNMP.csv\"\n",
    "path_DdoS_SSDP = \"D:\\\\thesis_dataset\\\\01-12\\\\DrDoS_SSDP.csv\"\n",
    "path_DdoS_UDP = \"D:\\\\thesis_dataset\\\\01-12\\\\DrDoS_UDP.csv\"\n",
    "path_Syn = \"D:\\\\thesis_dataset\\\\01-12\\\\Syn.csv\"\n",
    "path_TFTP = \"D:\\\\thesis_dataset\\\\01-12\\\\TFTP.csv\"\n",
    "path_UDPLag = \"D:\\\\thesis_dataset\\\\01-12\\\\UDPLag.csv\"\n",
    "\n",
    "# # CSV-03-11\n",
    "# path__LDAP = \"../CICDDoS-2019/CSV-03-11/03-11/LDAP.csv\"\n",
    "# path__MSSQL = \"../CICDDoS-2019/CSV-03-11/03-11/MSSQL.csv\"\n",
    "# path__NetBIOS = \"../CICDDoS-2019/CSV-03-11/03-11/NetBIOS.csv\"\n",
    "# path__Portmap = \"../CICDDoS-2019/CSV-03-11/03-11/Portmap.csv\"\n",
    "# path__Syn = \"../CICDDoS-2019/CSV-03-11/03-11/Syn.csv\"\n",
    "# path__UDP = \"../CICDDoS-2019/CSV-03-11/03-11/UDP.csv\"\n",
    "# path__UDPLag = \"../CICDDoS-2019/CSV-03-11/03-11/UDPLag.csv\"\n",
    "\n",
    "paths = [path_DdoS_DNS, path_DdoS_MSSQL, path_DdoS_LDAP, path_DdoS_NTP, path_DdoS_NetBIOS, path_DdoS_SNMP,\n",
    "         path_DdoS_SSDP, path_DdoS_UDP, path_Syn, path_TFTP, path_UDPLag]\n",
    "# , path__LDAP, path__MSSQL, path__NetBIOS,\n",
    "#      path__Portmap, path__Syn, path__UDP, path__UDPLag]\n",
    "print(\"Total Dataset CSV Files\", len(paths))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assuming Column / feature names might differ in different csv files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "col_name_consistency = {\n",
    "    'Flow ID': 'Flow ID',\n",
    "    'Source IP': 'Source IP',\n",
    "    'Src IP': 'Source IP',\n",
    "    'Source Port': 'Source Port',\n",
    "    'Src Port': 'Source Port',\n",
    "    'Destination IP': 'Destination IP',\n",
    "    'Dst IP': 'Destination IP',\n",
    "    'Destination Port': 'Destination Port',\n",
    "    'Dst Port': 'Destination Port',\n",
    "    'Protocol': 'Protocol',\n",
    "    'Timestamp': 'Timestamp',\n",
    "    'Flow Duration': 'Flow Duration',\n",
    "    'Total Fwd Packets': 'Total Fwd Packets',\n",
    "    'Tot Fwd Pkts': 'Total Fwd Packets',\n",
    "    'Total Backward Packets': 'Total Backward Packets',\n",
    "    'Tot Bwd Pkts': 'Total Backward Packets',\n",
    "    'Total Length of Fwd Packets': 'Fwd Packets Length Total',\n",
    "    'TotLen Fwd Pkts': 'Fwd Packets Length Total',\n",
    "    'Total Length of Bwd Packets': 'Bwd Packets Length Total',\n",
    "    'TotLen Bwd Pkts': 'Bwd Packets Length Total',\n",
    "    'Fwd Packet Length Max': 'Fwd Packet Length Max',\n",
    "    'Fwd Pkt Len Max': 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min': 'Fwd Packet Length Min',\n",
    "    'Fwd Pkt Len Min': 'Fwd Packet Length Min',\n",
    "    'Fwd Packet Length Mean': 'Fwd Packet Length Mean',\n",
    "    'Fwd Pkt Len Mean': 'Fwd Packet Length Mean',\n",
    "    'Fwd Packet Length Std': 'Fwd Packet Length Std',\n",
    "    'Fwd Pkt Len Std': 'Fwd Packet Length Std',\n",
    "    'Bwd Packet Length Max': 'Bwd Packet Length Max',\n",
    "    'Bwd Pkt Len Max': 'Bwd Packet Length Max',\n",
    "    'Bwd Packet Length Min': 'Bwd Packet Length Min',\n",
    "    'Bwd Pkt Len Min': 'Bwd Packet Length Min',\n",
    "    'Bwd Packet Length Mean': 'Bwd Packet Length Mean',\n",
    "    'Bwd Pkt Len Mean': 'Bwd Packet Length Mean',\n",
    "    'Bwd Packet Length Std': 'Bwd Packet Length Std',\n",
    "    'Bwd Pkt Len Std': 'Bwd Packet Length Std',\n",
    "    'Flow Bytes/s': 'Flow Bytes/s',\n",
    "    'Flow Byts/s': 'Flow Bytes/s',\n",
    "    'Flow Packets/s': 'Flow Packets/s',\n",
    "    'Flow Pkts/s': 'Flow Packets/s',\n",
    "    'Flow IAT Mean': 'Flow IAT Mean',\n",
    "    'Flow IAT Std': 'Flow IAT Std',\n",
    "    'Flow IAT Max': 'Flow IAT Max',\n",
    "    'Flow IAT Min': 'Flow IAT Min',\n",
    "    'Fwd IAT Total': 'Fwd IAT Total',\n",
    "    'Fwd IAT Tot': 'Fwd IAT Total',\n",
    "    'Fwd IAT Mean': 'Fwd IAT Mean',\n",
    "    'Fwd IAT Std': 'Fwd IAT Std',\n",
    "    'Fwd IAT Max': 'Fwd IAT Max',\n",
    "    'Fwd IAT Min': 'Fwd IAT Min',\n",
    "    'Bwd IAT Total': 'Bwd IAT Total',\n",
    "    'Bwd IAT Tot': 'Bwd IAT Total',\n",
    "    'Bwd IAT Mean': 'Bwd IAT Mean',\n",
    "    'Bwd IAT Std': 'Bwd IAT Std',\n",
    "    'Bwd IAT Max': 'Bwd IAT Max',\n",
    "    'Bwd IAT Min': 'Bwd IAT Min',\n",
    "    'Fwd PSH Flags': 'Fwd PSH Flags',\n",
    "    'Bwd PSH Flags': 'Bwd PSH Flags',\n",
    "    'Fwd URG Flags': 'Fwd URG Flags',\n",
    "    'Bwd URG Flags': 'Bwd URG Flags',\n",
    "    'Fwd Header Length': 'Fwd Header Length',\n",
    "    'Fwd Header Len': 'Fwd Header Length',\n",
    "    'Bwd Header Length': 'Bwd Header Length',\n",
    "    'Bwd Header Len': 'Bwd Header Length',\n",
    "    'Fwd Packets/s': 'Fwd Packets/s',\n",
    "    'Fwd Pkts/s': 'Fwd Packets/s',\n",
    "    'Bwd Packets/s': 'Bwd Packets/s',\n",
    "    'Bwd Pkts/s': 'Bwd Packets/s',\n",
    "    'Min Packet Length': 'Packet Length Min',\n",
    "    'Pkt Len Min': 'Packet Length Min',\n",
    "    'Max Packet Length': 'Packet Length Max',\n",
    "    'Pkt Len Max': 'Packet Length Max',\n",
    "    'Packet Length Mean': 'Packet Length Mean',\n",
    "    'Pkt Len Mean': 'Packet Length Mean',\n",
    "    'Packet Length Std': 'Packet Length Std',\n",
    "    'Pkt Len Std': 'Packet Length Std',\n",
    "    'Packet Length Variance': 'Packet Length Variance',\n",
    "    'Pkt Len Var': 'Packet Length Variance',\n",
    "    'FIN Flag Count': 'FIN Flag Count',\n",
    "    'FIN Flag Cnt': 'FIN Flag Count',\n",
    "    'SYN Flag Count': 'SYN Flag Count',\n",
    "    'SYN Flag Cnt': 'SYN Flag Count',\n",
    "    'RST Flag Count': 'RST Flag Count',\n",
    "    'RST Flag Cnt': 'RST Flag Count',\n",
    "    'PSH Flag Count': 'PSH Flag Count',\n",
    "    'PSH Flag Cnt': 'PSH Flag Count',\n",
    "    'ACK Flag Count': 'ACK Flag Count',\n",
    "    'ACK Flag Cnt': 'ACK Flag Count',\n",
    "    'URG Flag Count': 'URG Flag Count',\n",
    "    'URG Flag Cnt': 'URG Flag Count',\n",
    "    'CWE Flag Count': 'CWE Flag Count',\n",
    "    'CWE Flag Cnt': 'CWE Flag Count',\n",
    "    'ECE Flag Count': 'ECE Flag Count',\n",
    "    'ECE Flag Cnt': 'ECE Flag Count',\n",
    "    'Down/Up Ratio': 'Down/Up Ratio',\n",
    "    'Average Packet Size': 'Avg Packet Size',\n",
    "    'Pkt Size Avg': 'Avg Packet Size',\n",
    "    'Avg Fwd Segment Size': 'Avg Fwd Segment Size',\n",
    "    'Fwd Seg Size Avg': 'Avg Fwd Segment Size',\n",
    "    'Avg Bwd Segment Size': 'Avg Bwd Segment Size',\n",
    "    'Bwd Seg Size Avg': 'Avg Bwd Segment Size',\n",
    "    'Fwd Avg Bytes/Bulk': 'Fwd Avg Bytes/Bulk',\n",
    "    'Fwd Byts/b Avg': 'Fwd Avg Bytes/Bulk',\n",
    "    'Fwd Avg Packets/Bulk': 'Fwd Avg Packets/Bulk',\n",
    "    'Fwd Pkts/b Avg': 'Fwd Avg Packets/Bulk',\n",
    "    'Fwd Avg Bulk Rate': 'Fwd Avg Bulk Rate',\n",
    "    'Fwd Blk Rate Avg': 'Fwd Avg Bulk Rate',\n",
    "    'Bwd Avg Bytes/Bulk': 'Bwd Avg Bytes/Bulk',\n",
    "    'Bwd Byts/b Avg': 'Bwd Avg Bytes/Bulk',\n",
    "    'Bwd Avg Packets/Bulk': 'Bwd Avg Packets/Bulk',\n",
    "    'Bwd Pkts/b Avg': 'Bwd Avg Packets/Bulk',\n",
    "    'Bwd Avg Bulk Rate': 'Bwd Avg Bulk Rate',\n",
    "    'Bwd Blk Rate Avg': 'Bwd Avg Bulk Rate',\n",
    "    'Subflow Fwd Packets': 'Subflow Fwd Packets',\n",
    "    'Subflow Fwd Pkts': 'Subflow Fwd Packets',\n",
    "    'Subflow Fwd Bytes': 'Subflow Fwd Bytes',\n",
    "    'Subflow Fwd Byts': 'Subflow Fwd Bytes',\n",
    "    'Subflow Bwd Packets': 'Subflow Bwd Packets',\n",
    "    'Subflow Bwd Pkts': 'Subflow Bwd Packets',\n",
    "    'Subflow Bwd Bytes': 'Subflow Bwd Bytes',\n",
    "    'Subflow Bwd Byts': 'Subflow Bwd Bytes',\n",
    "    'Init_Win_bytes_forward': 'Init Fwd Win Bytes',\n",
    "    'Init Fwd Win Byts': 'Init Fwd Win Bytes',\n",
    "    'Init_Win_bytes_backward': 'Init Bwd Win Bytes',\n",
    "    'Init Bwd Win Byts': 'Init Bwd Win Bytes',\n",
    "    'act_data_pkt_fwd': 'Fwd Act Data Packets',\n",
    "    'Fwd Act Data Pkts': 'Fwd Act Data Packets',\n",
    "    'min_seg_size_forward': 'Fwd Seg Size Min',\n",
    "    'Fwd Seg Size Min': 'Fwd Seg Size Min',\n",
    "    'Active Mean': 'Active Mean',\n",
    "    'Active Std': 'Active Std',\n",
    "    'Active Max': 'Active Max',\n",
    "    'Active Min': 'Active Min',\n",
    "    'Idle Mean': 'Idle Mean',\n",
    "    'Idle Std': 'Idle Std',\n",
    "    'Idle Max': 'Idle Max',\n",
    "    'Idle Min': 'Idle Min',\n",
    "    'Label': 'Label'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using Parallel Processing {Dask Library} to Read the Large (above 1GB) csv files as chunk-wise but returning pandas dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def readHugeCsvFileAsDataFrame(file_path):\n",
    "    dtypes = {'SimillarHTTP': 'object',\n",
    "              'Timestamp': 'object',\n",
    "              'Source IP': 'str',\n",
    "              'Destination IP': 'str',\n",
    "              'Flow ID': 'object',\n",
    "              'Label': 'object',\n",
    "              }\n",
    "    for feature in [f'f_{i}' for i in range(82)]:\n",
    "        dtypes[feature] = \"float32\"\n",
    "\n",
    "    dask_df = dd.read_csv(file_path, low_memory=False, blocksize=50000, dtype=dtypes)  # 50MB chunk-size\n",
    "    return dask_df.compute()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With Scheduler (see the elapsed time to process each file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "daskScheduler = Client()\n",
    "worker_output = daskScheduler.submit(readHugeCsvFileAsDataFrame, path_UDPLag)\n",
    "workingDataFrame = worker_output.result()\n",
    "workingDataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mb = workingDataFrame.memory_usage().sum() / 1024**2\n",
    "print('Memory usage of Working Dataframe: {:.2f} MB'.format(mb))\n",
    "print(\"Total Features in the initial csv file: \", len(list(workingDataFrame.columns)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h4 style=\"background:#ADFF2F;color:black\">\n",
    "Following Columns may have little insignificance over model creation. Follow <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-09484-2_2\" >This Paper Link</a>\n",
    "</h4></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# src_port,dst_port, src_ip, dst_ip these are duplicate\n",
    "drop_columns = [\n",
    "    \"Flow ID\",\n",
    "    'Fwd Header Length.1',\n",
    "    \"Source IP\", \"Src IP\",\n",
    "    \"Source Port\", \"Src Port\",\n",
    "    \"Destination IP\", \"Dst IP\",\n",
    "    \"Destination Port\", \"Dst Port\",\n",
    "    \"Timestamp\",\n",
    "    \"Unnamed: 0\",\n",
    "    \"Inbound\",\n",
    "    \"SimillarHTTP\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h3 style=\"background:yellow;color:black\">\n",
    "Dropping Unnecessary Features & maintain column consistency\n",
    "</h3></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.columns = workingDataFrame.columns.str.strip()  # sometimes there's leading / trailing whitespace\n",
    "workingDataFrame.rename(columns=col_name_consistency, inplace=True)\n",
    "workingDataFrame.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "\n",
    "workingDataFrame.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h3 style=\"background:yellow;color:black\">\n",
    "Data Cleaning Based on Data Types (DownSizing)\n",
    "</h3></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.info(memory_usage=\"deep\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# converting 64bit float to 32 bit float & 64bit integer to 8bit integer\n",
    "for column in workingDataFrame:\n",
    "    if workingDataFrame[column].dtype == 'float64':\n",
    "        workingDataFrame[column] = pd.to_numeric(workingDataFrame[column], downcast='float')\n",
    "    if workingDataFrame[column].dtype == 'int64':\n",
    "        workingDataFrame[column] = pd.to_numeric(workingDataFrame[column], downcast='integer')\n",
    "\n",
    "workingDataFrame.info(memory_usage=\"deep\") # observe the dTypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h3 style=\"background:yellow;color:black\">\n",
    "Removing NaN values & infinite values\n",
    "</h3></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# workingDataFrame.isnull().sum()\n",
    "print(f\"pure missing value contains rows: {workingDataFrame.isna().any(axis=1).sum()}\")\n",
    "workingDataFrame.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "print(f\"missing value contains total rows: {workingDataFrame.isna().any(axis=1).sum()}\")\n",
    "workingDataFrame.dropna(inplace=True)\n",
    "print(f\"After dropping NaN values, number of rows: {workingDataFrame.shape[0]}\")\n",
    "print(f\"After dropping NaN values, number of columns: {workingDataFrame.shape[1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h3 style=\"background:yellow;color:black\">\n",
    "Dropping Duplicate Rows\n",
    "</h3></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Fully duplicate total rows: {workingDataFrame.duplicated().sum()}\")\n",
    "workingDataFrame.drop_duplicates(inplace=True)\n",
    "workingDataFrame.reset_index(inplace=True, drop=True)\n",
    "print(f\"After dropping Duplicate values, number of rows: {workingDataFrame.shape[0]}\")\n",
    "print(f\"After dropping Duplicate values, number of columns: {workingDataFrame.shape[1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h3 style=\"background:#FF00FF;color:black\">\n",
    "After data cleaning, how much storage it holds now\n",
    "</h3></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.info(memory_usage=\"deep\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h3 style=\"background:#00FF00;color:black\">\n",
    "Saving New DataFrame as csv file to new location\n",
    "</h3></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dataCleaningResultToAnotherCSV(dataFrameArg, dirPath, file_name):\n",
    "    dataFrameArg.to_csv(dirPath + file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newCsvPath = \"D:\\\\thesis_dataset\\\\01-12\\\\AfterDataCleaning(Final)\\\\\"\n",
    "newFileName = \"cleaned_udpLag.csv\"\n",
    "# newFileName = \"cleaned_Syn.csv\"\n",
    "# newFileName = \"cleaned_DrDoS_NTP.csv\"\n",
    "# newFileName = \"cleaned_DrDoS_LDAP.csv\"\n",
    "# newFileName = \"cleaned_DrDoS_SSDP.csv\"\n",
    "# newFileName = \"cleaned_DrDoS_UDP.csv\"\n",
    "dataCleaningResultToAnotherCSV(dataFrameArg=workingDataFrame, dirPath=newCsvPath, file_name=newFileName)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
